# utils/db_helpers.py
"""
Database helper utilities for the tender-extraction pipeline.

Requirements:
    pip install pymysql

This module provides:
 - get_db_conn(): simple connection factory (dict cursor)
 - fetch_pending_batch(batch_size): read candidate rows (todayscan=0)
 - claim_batch(lock_id, batch_size): atomically claim N rows for processing by setting locked_by and todayscan->2 (processing)
 - mark_done(bid_number, updates): mark a row done (todayscan=1) and write pdf/json paths, processed_at, status
 - mark_error(bid_number, err_msg): increment attempts, set status='error' and optionally release lock
 - release_stale_locks(max_age_seconds): optional housekeeping to free stuck rows
 - simple helper to run arbitrary SQL: execute(), fetchall()

Design notes:
 - Uses optimistic claiming with an atomic UPDATE ... LIMIT pattern.
 - Claims set todayscan = 2 (processing); consumers should set todayscan = 1 when finished.
 - locked_by should be a string id (e.g., worker hostname + uuid). Pass lock_id from worker.
"""

import time
import uuid
import socket
from typing import List, Dict, Any, Optional, Tuple

import pymysql
import pymysql.cursors
from config import MYSQL

DB = MYSQL  # expected dict with host, port, user, password, db, charset

# tweakable
CLAIM_PROCESSING_VALUE = 2  # temporary value for rows that are being processed
MAX_ATTEMPTS = 5


def get_db_conn():
    """Return a pymysql connection (caller should close it)."""
    conn = pymysql.connect(
        host=DB.get("host", "127.0.0.1"),
        port=int(DB.get("port", 3306)),
        user=DB.get("user", "root"),
        password=DB.get("password", ""),
        db=DB.get("db", "gem_extractor"),
        charset=DB.get("charset", "utf8mb4"),
        cursorclass=pymysql.cursors.DictCursor,
        autocommit=False,
    )
    return conn


# --------------------
# low-level helpers
# --------------------
def execute(sql: str, params: Tuple = (), commit: bool = False) -> int:
    """
    Execute a non-select statement. Returns affected rowcount.
    """
    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            cur.execute(sql, params)
            if commit:
                conn.commit()
            return cur.rowcount
    finally:
        conn.close()


def fetchall(sql: str, params: Tuple = ()) -> List[Dict[str, Any]]:
    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            cur.execute(sql, params)
            return cur.fetchall()
    finally:
        conn.close()


# --------------------
# producer / claim logic
# --------------------
def fetch_pending_batch_preview(limit: int = 100) -> List[Dict[str, Any]]:
    """
    Read a preview of pending rows (todayscan = 0). This does NOT claim them.
    Useful for UI or dry-run.
    """
    sql = "SELECT id, bid_number, detail_url FROM bids WHERE todayscan = 0 LIMIT %s"
    return fetchall(sql, (limit,))


def claim_batch(lock_id: Optional[str] = None, batch_size: int = 200) -> List[Dict[str, Any]]:
    """
    Atomically claim up to batch_size rows for processing.
    Returns the claimed rows (list of dicts).
    Claiming logic:
      1. UPDATE bids SET locked_by = <lock_id>, todayscan = CLAIM_PROCESSING_VALUE, status='processing'
         WHERE todayscan = 0 AND (locked_by IS NULL OR locked_by = '') LIMIT batch_size
      2. SELECT the rows with locked_by = lock_id and todayscan = CLAIM_PROCESSING_VALUE
    Notes:
      - MySQL's UPDATE ... LIMIT is used; this is atomic on a single connection.
      - lock_id: if None, autogenerated as host-uuid-timestamp.
    """
    if not lock_id:
        lock_id = f"{socket.gethostname()}_{uuid.uuid4().hex}_{int(time.time())}"

    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            # 1) atomically claim
            update_sql = (
                "UPDATE bids "
                "SET locked_by = %s, todayscan = %s, status = 'processing' "
                "WHERE todayscan = 0 AND (locked_by IS NULL OR locked_by = '') "
                "LIMIT %s"
            )
            cur.execute(update_sql, (lock_id, CLAIM_PROCESSING_VALUE, batch_size))
            claimed = cur.rowcount
            conn.commit()

            if claimed == 0:
                return []

            # 2) fetch rows claimed by this lock_id
            sel_sql = "SELECT id, bid_number, detail_url, page FROM bids WHERE locked_by = %s AND todayscan = %s"
            cur.execute(sel_sql, (lock_id, CLAIM_PROCESSING_VALUE))
            rows = cur.fetchall()
            return rows
    finally:
        conn.close()


# --------------------
# consumer update helpers
# --------------------
def mark_done(bid_number: str, pdf_path: Optional[str] = None, json_path: Optional[str] = None, parse_confidence: Optional[float] = None, lock_id: Optional[str] = None):
    """
    Mark a bid as processed successfully.
    Sets todayscan = 1, status = 'done', processed_at = NOW(), clears locked_by, updates pdf/json paths & parse_confidence.
    """
    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            sql = (
                "UPDATE bids SET todayscan = 1, status = %s, processed_at = NOW(), locked_by = NULL, attempts = 0"
            )
            params = ["done"]

            if pdf_path is not None:
                sql += ", pdf_path = %s"
                params.append(pdf_path)
            if json_path is not None:
                sql += ", json_path = %s"
                params.append(json_path)
            if parse_confidence is not None:
                sql += ", parse_confidence = %s"
                params.append(parse_confidence)

            sql += " WHERE bid_number = %s"
            params.append(bid_number)

            cur.execute(sql, tuple(params))
            conn.commit()
            return cur.rowcount
    finally:
        conn.close()


def mark_error(bid_number: str, err_msg: Optional[str] = None, increment_attempts: bool = True, release_lock: bool = False):
    """
    Mark a row as errored.
     - increments attempts (if increment_attempts True)
     - sets status = 'error' and stores last error in status or json_path? (we avoid new columns)
     - optionally releases the lock (locked_by = NULL) so another worker can pick it up.
    """
    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            if increment_attempts:
                cur.execute("UPDATE bids SET attempts = attempts + 1 WHERE bid_number = %s", (bid_number,))
            # update status and optionally release lock
            if release_lock:
                cur.execute("UPDATE bids SET status = %s, locked_by = NULL WHERE bid_number = %s", ("error", bid_number))
            else:
                cur.execute("UPDATE bids SET status = %s WHERE bid_number = %s", ("error", bid_number))
            conn.commit()
            return True
    finally:
        conn.close()


def rerelease_batch_for_lock(lock_id: str):
    """
    Release all rows held by a given lock id (set locked_by = NULL, todayscan = 0, status = 'new') - used for graceful shutdown or failures.
    """
    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            cur.execute("UPDATE bids SET locked_by = NULL, todayscan = 0, status = 'new' WHERE locked_by = %s", (lock_id,))
            affected = cur.rowcount
            conn.commit()
            return affected
    finally:
        conn.close()


def release_stale_locks(max_age_seconds: int = 3600) -> int:
    """
    Release locks that have been in 'processing' state for longer than max_age_seconds.
    This uses processed_at/created_at heuristics: if created_at or processed_at is older than threshold and todayscan = CLAIM_PROCESSING_VALUE, reset it.
    Returns number of rows released.
    """
    cutoff = int(time.time()) - max_age_seconds
    # convert cutoff to datetime string in MySQL-friendly form
    cutoff_dt = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(cutoff))
    conn = get_db_conn()
    try:
        with conn.cursor() as cur:
            # Reset rows that have been processing too long
            cur.execute(
                "UPDATE bids SET locked_by = NULL, todayscan = 0, status = 'new' "
                "WHERE todayscan = %s AND (processed_at IS NULL OR processed_at < %s)",
                (CLAIM_PROCESSING_VALUE, cutoff_dt)
            )
            affected = cur.rowcount
            conn.commit()
            return affected
    finally:
        conn.close()


# --------------------
# small utility
# --------------------
def get_stats() -> Dict[str, Any]:
    """
    Return quick counts: total, to_process, processing, done, errors
    """
    sql = """
    SELECT
      (SELECT COUNT(*) FROM bids) AS total,
      (SELECT COUNT(*) FROM bids WHERE todayscan = 0) AS to_process,
      (SELECT COUNT(*) FROM bids WHERE todayscan = %s) AS processing,
      (SELECT COUNT(*) FROM bids WHERE todayscan = 1) AS done,
      (SELECT COUNT(*) FROM bids WHERE status = 'error') AS errors
    """
    rows = fetchall(sql, (CLAIM_PROCESSING_VALUE,))
    return rows[0] if rows else {}


# --------------------
# Example usage (for reference only; do not run on import)
# --------------------
if __name__ == "__main__":
    print("Quick DB stats:", get_stats())
    # claim 10 rows
    lid = f"{socket.gethostname()}_{uuid.uuid4().hex}"
    claimed = claim_batch(lock_id=lid, batch_size=10)
    print("Claimed rows:", len(claimed))
    for r in claimed:
        print(r)
    # simulate marking one done
    if claimed:
        bn = claimed[0]["bid_number"]
        mark_done(bn, pdf_path="PDF/example.pdf", json_path="OUTPUT/example.json")
        print("Marked done:", bn)
